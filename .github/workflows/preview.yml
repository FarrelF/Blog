name: Testing Site
on: 
  pull_request:
    branches:
      - drafts
      - main
    paths-ignore:
      - "**/README.md"
      - "**/LICENSE"
      - "**/LICENSE*"
      - "**/CONTRIBUTING.md"
      - "**/.gitignore"
      - "**/netlify.toml"
      - "config/development/**"
      - "**/archetypes/**"

jobs:
  build:
    name: Build (Preview)
    runs-on: ubuntu-20.04
    env:
      TZ: Asia/Jakarta
      HUGO_ENV: staging

    steps:
    - name: Checkout the Code
      uses: actions/checkout@v2
      with:
        fetch-depth: 0

    - name: Install and Set up Go
      uses: actions/setup-go@v2
      with:
        go-version: '^1.17.3'

    - name: Install and Set up Locales and Other Packages
      run: |
        # case "$(curl -s --max-time 3 -I http://azure.archive.ubuntu.com | sed 's/^[^ ]*  *\([0-9]\).*/\1/; 1q')" in
        #   [23]) echo "The Repository HTTP Server is up"
        #         if nc -zw3 azure.archive.ubuntu.com 80; then
        #           echo "The Repository Server has been connected successfully"
        #         else
        #           echo "The Repository server is down, changing the Repository Server. Please wait....."
        #           sudo sed -i 's|http://azure.archive.ubuntu.com/ubuntu/|https://mirror.b-cdn.net/ubuntu/|g' /etc/apt/sources.list
        #           sudo sed -i 's|http://security.ubuntu.com/ubuntu|https://mirror.b-cdn.net/ubuntu/|g' /etc/apt/sources.list
        #           sudo sed -i 's|http://ppa.launchpad.net|https://mirrorppa.b-cdn.net|g' /etc/apt/sources.list.d/*.list
        #         fi;;
        #   *) echo "The Repository HTTP Server is down, changing the Repository Server. Please wait....."
        #      sudo sed -i 's|http://azure.archive.ubuntu.com/ubuntu/|https://mirror.b-cdn.net/ubuntu/|g' /etc/apt/sources.list
        #      sudo sed -i 's|http://security.ubuntu.com/ubuntu|https://mirror.b-cdn.net/ubuntu/|g' /etc/apt/sources.list
        #      sudo sed -i 's|http://ppa.launchpad.net|https://mirrorppa.b-cdn.net|g' /etc/apt/sources.list.d/*.list;;
        # esac
        sudo sh -c 'apt update; apt install -y locales locales-all pigz'
        sudo sed -i 's/id_ID\s.*$/id_ID id_ID.utf8/g' /usr/share/locale/locale.alias
        sudo sed -i 's/# id_ID\.UTF-8/id_ID\.UTF-8/' /etc/locale.gen
        sudo update-locale LANG=C.UTF-8 LC_MESSAGES=POSIX
        DEBIAN_FRONTEND=noninteractive sudo -E dpkg-reconfigure locales
        sudo ln -sf /usr/share/zoneinfo/${TZ} /etc/localtime

    - name: Install and Set up Hugo Extended
      run: |
        HUGO_VERSION="$(curl -s https://api.github.com/repos/gohugoio/hugo/releases/latest | grep tag_name | cut -d 'v' -f2 | cut -d'"' -f1)"
        cd /tmp/
        wget https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_Linux-64bit.tar.gz
        tar -xzf hugo_extended_${HUGO_VERSION}_Linux-64bit.tar.gz
        chmod +x hugo
        sudo -- sh -c 'mv hugo /usr/local/bin/; chown root:root /usr/local/bin/hugo'
        rm hugo_extended_${HUGO_VERSION}_Linux-64bit.tar.gz LICENSE README.md
        cd ${GITHUB_WORKSPACE}
        which hugo && hugo version
        # find ./content -type f -exec file {} \; | awk -F: '{ if ($2 ~/[Ii]mage|EPS/) print $1}' > Images.list
        # sha256sum $(cat Images.list) > Images.hash.list
        # git ls-tree -rtl HEAD:layouts --full-tree > layouts-tree.hash

    - name: Set up Hugo Resources Cache
      uses: actions/cache@v2
      with:
        path: |
          ./resources
        key: v2-5-hugo-preview
        restore-keys: | 
          - v2-5-hugo-preview

    - name: Set up Hugo Modules Cache
      uses: actions/cache@v2
      with:
        path: |
          ./cache/modules
        key: v1-1-hugo-modules-${{ hashFiles('**/go.mod') }}-${{ hashFiles('**/go.sum') }}
        restore-keys: | 
          - v1-1-hugo-modules-${{ hashFiles('**/go.mod') }}-${{ hashFiles('**/go.sum') }}
          - v1-1-hugo-modules-${{ hashFiles('**/go.mod') }}-

    - name: Building the Blog HTML (Preview)
      run: |
        rm -rf static/*.txt static/*.xml static/google*.html static/CNAME static/_redirects
        hugo --gc --minify --buildFuture --environment staging --cacheDir ${PWD}/cache -b https://deploy-preview.farrel.franqois.id

    - name: Compressing Static Web Files
      run: |
        pigz -km -9 $(find public -iname '*.html' -o -iname '*.css' -o -iname '*.js' -o -iname '*.json' -o -iname '*.xml' -o -iname '*.svg' -o -iname '*.txt')
        brotli -Zn $(find public -iname '*.html' -o -iname '*.css' -o -iname '*.js' -o -iname '*.json' -o -iname '*.xml' -o -iname '*.svg' -o -iname '*.txt')
        tar cvf - ./public | pigz -9 > files.tar.gz

    - name: Upload Output File to Artifact
      uses: actions/upload-artifact@v2
      with:
        name: site-outputs
        path: files.tar.gz
        retention-days: 1

  deploy_to_bunny_net:
    name: Deploy to Bunny.net Storage 
    needs: build
    runs-on: ubuntu-20.04
    steps:
    - name: Download Output File from Artifact
      uses: actions/download-artifact@v2
      with:
        name: site-outputs

    - name: Decompressing Files
      run: |
        tar -xvzf files.tar.gz
    
    - name: Deploying to Bunny.net Storage
      uses: own3d/bunny-action@main
      env:
        BUNNY_API_ACCESS_KEY: ${{ secrets.BUNNY_API_KEY }}
        BUNNY_STORAGE_USERNAME: ${{ secrets.BUNNY_PREVIEW_STORAGE_USERNAME }}
        BUNNY_STORAGE_PASSWORD: ${{ secrets.BUNNY_PREVIEW_STORAGE_PASSWORD }}
        BUNNY_PULL_ZONE_ID: ${{ secrets.BUNNY_PREVIEW_PULLZONE_ID }}
      with:
        args: deploy --dir=./public

  deploy_to_storj-ap1-s3:
    name: Deploy to Storj DCS AP1 S3 Storage (Preview)
    needs: build
    runs-on: ubuntu-20.04
    steps:
    - name: Download Output File from Artifact
      uses: actions/download-artifact@v2
      with:
        name: site-outputs

    - name: Decompressing Files
      run: |
        tar -xvzf files.tar.gz

    - name: Installing and Configuring RClone
      run: |
        curl https://rclone.org/install.sh | sudo bash
        mkdir -p ${HOME}/.config/rclone
        echo "${{ secrets.RCLONE_CONFIG }}" > ${HOME}/.config/rclone/rclone.conf

    - name: Deploying to S3 Storage
      env:
        RCLONE_CONFIG_PASS: ${{ secrets.RCLONE_CONFIG_PASS }}
        S3_SERVICE: storj-ap1-s3
      run: |
        rclone sync -v -P --stats-one-line --checksum --transfers=64 --checkers=64 --fast-list ./public ${S3_SERVICE}:/${{ secrets.S3_PREVIEW_STORAGE_BUCKET }}

  deploy_to_storj-us1-s3:
    name: Deploy to Storj DCS US1 S3 Storage (Preview)
    needs: build
    runs-on: ubuntu-20.04
    steps:
    - name: Download Output File from Artifact
      uses: actions/download-artifact@v2
      with:
        name: site-outputs

    - name: Decompressing Files
      run: |
        tar -xvzf files.tar.gz

    - name: Installing and Configuring RClone
      run: |
        curl https://rclone.org/install.sh | sudo bash
        mkdir -p ${HOME}/.config/rclone
        echo "${{ secrets.RCLONE_CONFIG }}" > ${HOME}/.config/rclone/rclone.conf

    - name: Deploying to S3 Storage
      env:
        RCLONE_CONFIG_PASS: ${{ secrets.RCLONE_CONFIG_PASS }}
        S3_SERVICE: storj-us1-s3
      run: |
        rclone sync -v -P --stats-one-line --checksum --transfers=64 --checkers=64 --fast-list ./public ${S3_SERVICE}:/${{ secrets.S3_PREVIEW_STORAGE_BUCKET }}

  deploy_to_storj-eu1-s3:
    name: Deploy to Storj DCS EU1 S3 Storage (Preview)
    needs: build
    runs-on: ubuntu-20.04
    steps:
    - name: Download Output File from Artifact
      uses: actions/download-artifact@v2
      with:
        name: site-outputs

    - name: Decompressing Files
      run: |
        tar -xvzf files.tar.gz

    - name: Installing and Configuring RClone
      run: |
        curl https://rclone.org/install.sh | sudo bash
        mkdir -p ${HOME}/.config/rclone
        echo "${{ secrets.RCLONE_CONFIG }}" > ${HOME}/.config/rclone/rclone.conf

    - name: Deploying to S3 Storage
      env:
        RCLONE_CONFIG_PASS: ${{ secrets.RCLONE_CONFIG_PASS }}
        S3_SERVICE: storj-eu1-s3
      run: |
        rclone sync -v -P --stats-one-line --checksum --transfers=64 --checkers=64 --fast-list ./public ${S3_SERVICE}:/${{ secrets.S3_PREVIEW_STORAGE_BUCKET }}
